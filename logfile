Using SCRIPTS_ROOTDIR: /home/hgkumbhare/mosesdecoder/scripts
Using single-thread GIZA
using gzip 
(1) preparing corpus @ Sun Apr 23 12:17:48 IST 2017
Executing: mkdir -p /home/hgkumbhare/working/train/corpus
(1.0) selecting factors @ Sun Apr 23 12:17:48 IST 2017
(1.1) running mkcls  @ Sun Apr 23 12:17:48 IST 2017
/home/hgkumbhare/mosesdecoder/tools/mkcls -c50 -n2 -p/home/hgkumbhare/corpus/nl_file_lowercase.fr -V/home/hgkumbhare/working/train/corpus/fr.vcb.classes opt
Executing: /home/hgkumbhare/mosesdecoder/tools/mkcls -c50 -n2 -p/home/hgkumbhare/corpus/nl_file_lowercase.fr -V/home/hgkumbhare/working/train/corpus/fr.vcb.classes opt

***** 2 runs. (algorithm:TA)*****
;KategProblem:cats: 50   words: 146

start-costs: MEAN: 6887.35 (6800.64-6974.06)  SIGMA:86.7089   
  end-costs: MEAN: 5686.01 (5685.03-5686.99)  SIGMA:0.980147   
   start-pp: MEAN: 12.1161 (11.2486-12.9836)  SIGMA:0.867474   
     end-pp: MEAN: 4.4741 (4.47047-4.47772)  SIGMA:0.00362719   
 iterations: MEAN: 4390 (4322-4458)  SIGMA:68   
       time: MEAN: 0.0475765 (0.046942-0.048211)  SIGMA:0.0006345   
(1.1) running mkcls  @ Sun Apr 23 12:17:48 IST 2017
/home/hgkumbhare/mosesdecoder/tools/mkcls -c50 -n2 -p/home/hgkumbhare/corpus/nl_file_lowercase.en -V/home/hgkumbhare/working/train/corpus/en.vcb.classes opt
Executing: /home/hgkumbhare/mosesdecoder/tools/mkcls -c50 -n2 -p/home/hgkumbhare/corpus/nl_file_lowercase.en -V/home/hgkumbhare/working/train/corpus/en.vcb.classes opt

***** 2 runs. (algorithm:TA)*****
;KategProblem:cats: 50   words: 156

start-costs: MEAN: 6019.41 (5968.6-6070.22)  SIGMA:50.8077   
  end-costs: MEAN: 5130.34 (5129.02-5131.65)  SIGMA:1.31924   
   start-pp: MEAN: 11.808 (11.2688-12.3471)  SIGMA:0.539135   
     end-pp: MEAN: 5.30265 (5.29636-5.30894)  SIGMA:0.0062909   
 iterations: MEAN: 4702.5 (4647-4758)  SIGMA:55.5   
       time: MEAN: 0.055394 (0.054334-0.056454)  SIGMA:0.00106   
(1.2) creating vcb file /home/hgkumbhare/working/train/corpus/fr.vcb @ Sun Apr 23 12:17:48 IST 2017
(1.2) creating vcb file /home/hgkumbhare/working/train/corpus/en.vcb @ Sun Apr 23 12:17:48 IST 2017
(1.3) numberizing corpus /home/hgkumbhare/working/train/corpus/fr-en-int-train.snt @ Sun Apr 23 12:17:48 IST 2017
(1.3) numberizing corpus /home/hgkumbhare/working/train/corpus/en-fr-int-train.snt @ Sun Apr 23 12:17:48 IST 2017
(2) running giza @ Sun Apr 23 12:17:48 IST 2017
(2.1a) running snt2cooc fr-en @ Sun Apr 23 12:17:48 IST 2017

Executing: mkdir -p /home/hgkumbhare/working/train/giza.fr-en
Executing: /home/hgkumbhare/mosesdecoder/tools/snt2cooc.out /home/hgkumbhare/working/train/corpus/en.vcb /home/hgkumbhare/working/train/corpus/fr.vcb /home/hgkumbhare/working/train/corpus/fr-en-int-train.snt > /home/hgkumbhare/working/train/giza.fr-en/fr-en.cooc
/home/hgkumbhare/mosesdecoder/tools/snt2cooc.out /home/hgkumbhare/working/train/corpus/en.vcb /home/hgkumbhare/working/train/corpus/fr.vcb /home/hgkumbhare/working/train/corpus/fr-en-int-train.snt > /home/hgkumbhare/working/train/giza.fr-en/fr-en.cooc
END.
(2.1b) running giza fr-en @ Sun Apr 23 12:17:48 IST 2017
/home/hgkumbhare/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/hgkumbhare/working/train/giza.fr-en/fr-en.cooc -c /home/hgkumbhare/working/train/corpus/fr-en-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/hgkumbhare/working/train/giza.fr-en/fr-en -onlyaldumps 1 -p0 0.999 -s /home/hgkumbhare/working/train/corpus/en.vcb -t /home/hgkumbhare/working/train/corpus/fr.vcb
Executing: /home/hgkumbhare/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/hgkumbhare/working/train/giza.fr-en/fr-en.cooc -c /home/hgkumbhare/working/train/corpus/fr-en-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/hgkumbhare/working/train/giza.fr-en/fr-en -onlyaldumps 1 -p0 0.999 -s /home/hgkumbhare/working/train/corpus/en.vcb -t /home/hgkumbhare/working/train/corpus/fr.vcb
/home/hgkumbhare/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/hgkumbhare/working/train/giza.fr-en/fr-en.cooc -c /home/hgkumbhare/working/train/corpus/fr-en-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/hgkumbhare/working/train/giza.fr-en/fr-en -onlyaldumps 1 -p0 0.999 -s /home/hgkumbhare/working/train/corpus/en.vcb -t /home/hgkumbhare/working/train/corpus/fr.vcb
Parameter 'coocurrencefile' changed from '' to '/home/hgkumbhare/working/train/giza.fr-en/fr-en.cooc'
Parameter 'c' changed from '' to '/home/hgkumbhare/working/train/corpus/fr-en-int-train.snt'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '117-04-23.121748.hgkumbhare' to '/home/hgkumbhare/working/train/giza.fr-en/fr-en'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to '/home/hgkumbhare/working/train/corpus/en.vcb'
Parameter 't' changed from '' to '/home/hgkumbhare/working/train/corpus/fr.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 117-04-23.121748.hgkumbhare.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/hgkumbhare/working/train/giza.fr-en/fr-en  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/hgkumbhare/working/train/corpus/fr-en-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/hgkumbhare/working/train/corpus/en.vcb  (source vocabulary file name)
t = /home/hgkumbhare/working/train/corpus/fr.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 117-04-23.121748.hgkumbhare.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/hgkumbhare/working/train/giza.fr-en/fr-en  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/hgkumbhare/working/train/corpus/fr-en-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/hgkumbhare/working/train/corpus/en.vcb  (source vocabulary file name)
t = /home/hgkumbhare/working/train/corpus/fr.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:/home/hgkumbhare/working/train/corpus/en.vcb
Reading vocabulary file from:/home/hgkumbhare/working/train/corpus/fr.vcb
Source vocabulary list has 157 unique tokens 
Target vocabulary list has 147 unique tokens 
Calculating vocabulary frequencies from corpus /home/hgkumbhare/working/train/corpus/fr-en-int-train.snt
Reading more sentence pairs into memory ... 
Corpus fits in memory, corpus has: 44 sentence pairs.
 Train total # sentence pairs (weighted): 44
Size of source portion of the training corpus: 1068 tokens
Size of the target portion of the training corpus: 1165 tokens 
In source portion of the training corpus, only 156 unique tokens appeared
In target portion of the training corpus, only 145 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 1165/(1112-44)== 1.09082
There are 5589 5589 entries in table
==========================================================
Model1 Training Started at: Sun Apr 23 12:17:48 2017

-----------
Model1: Iteration 1
Model1: (1) TRAIN CROSS-ENTROPY 7.35102 PERPLEXITY 163.26
Model1: (1) VITERBI TRAIN CROSS-ENTROPY 12.0726 PERPLEXITY 4307.26
Model 1 Iteration: 1 took: 0 seconds
-----------
Model1: Iteration 2
Model1: (2) TRAIN CROSS-ENTROPY 4.94605 PERPLEXITY 30.8255
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 8.7246 PERPLEXITY 423.025
Model 1 Iteration: 2 took: 0 seconds
-----------
Model1: Iteration 3
Model1: (3) TRAIN CROSS-ENTROPY 4.71688 PERPLEXITY 26.2981
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 8.11079 PERPLEXITY 276.434
Model 1 Iteration: 3 took: 0 seconds
-----------
Model1: Iteration 4
Model1: (4) TRAIN CROSS-ENTROPY 4.58501 PERPLEXITY 24.0008
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 7.60187 PERPLEXITY 194.264
Model 1 Iteration: 4 took: 0 seconds
-----------
Model1: Iteration 5
Model1: (5) TRAIN CROSS-ENTROPY 4.50286 PERPLEXITY 22.6722
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 7.23311 PERPLEXITY 150.447
Model 1 Iteration: 5 took: 0 seconds
Entire Model1 Training took: 0 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 156  #classes: 51
Read classes: #words: 146  #classes: 51

==========================================================
Hmm Training Started at: Sun Apr 23 12:17:48 2017

-----------
Hmm: Iteration 1
A/D table contains 19565 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 4.44881 PERPLEXITY 21.8386
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY 6.96626 PERPLEXITY 125.041

Hmm Iteration: 1 took: 0 seconds

-----------
Hmm: Iteration 2
A/D table contains 19565 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 4.21782 PERPLEXITY 18.6076
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 5.44938 PERPLEXITY 43.6944

Hmm Iteration: 2 took: 0 seconds

-----------
Hmm: Iteration 3
A/D table contains 19565 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 3.68561 PERPLEXITY 12.8671
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 4.4135 PERPLEXITY 21.3106

Hmm Iteration: 3 took: 0 seconds

-----------
Hmm: Iteration 4
A/D table contains 19565 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 2.83033 PERPLEXITY 7.11236
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 3.16013 PERPLEXITY 8.9391

Hmm Iteration: 4 took: 0 seconds

-----------
Hmm: Iteration 5
A/D table contains 19565 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 2.05097 PERPLEXITY 4.14385
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 2.23156 PERPLEXITY 4.69641

Hmm Iteration: 5 took: 0 seconds

Entire Hmm Training took: 0 seconds
==========================================================
Read classes: #words: 156  #classes: 51
Read classes: #words: 146  #classes: 51
Read classes: #words: 156  #classes: 51
Read classes: #words: 146  #classes: 51

==========================================================
Starting H333444:  Viterbi Training
 H333444 Training Started at: Sun Apr 23 12:17:48 2017


---------------------
THTo3: Iteration 1
#centers(pre/hillclimbed/real): 1 1 1  #al: 1083.91 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 19565 parameters.
A/D table contains 17036 parameters.
NTable contains 1570 parameter.
p0_count is 990.622 and p1 is 87.1889; p0 is 0.999 p1: 0.001
THTo3: TRAIN CROSS-ENTROPY 1.64821 PERPLEXITY 3.13444
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 1.71226 PERPLEXITY 3.27675

THTo3 Viterbi Iteration : 1 took: 0 seconds

---------------------
Model3: Iteration 2
#centers(pre/hillclimbed/real): 1 1 1  #al: 1084.57 #alsophisticatedcountcollection: 0 #hcsteps: 2.29545
#peggingImprovements: 0
A/D table contains 19565 parameters.
A/D table contains 17036 parameters.
NTable contains 1570 parameter.
p0_count is 1068.31 and p1 is 48.3463; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 1.97485 PERPLEXITY 3.93088
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 2.01495 PERPLEXITY 4.04166

Model3 Viterbi Iteration : 2 took: 1 seconds

---------------------
Model3: Iteration 3
#centers(pre/hillclimbed/real): 1 1 1  #al: 1084.45 #alsophisticatedcountcollection: 0 #hcsteps: 2.68182
#peggingImprovements: 0
A/D table contains 19565 parameters.
A/D table contains 17036 parameters.
NTable contains 1570 parameter.
p0_count is 1100.02 and p1 is 32.492; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 1.77941 PERPLEXITY 3.43285
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 1.81453 PERPLEXITY 3.51745

Model3 Viterbi Iteration : 3 took: 0 seconds

---------------------
T3To4: Iteration 4
#centers(pre/hillclimbed/real): 1 1 1  #al: 1084.25 #alsophisticatedcountcollection: 15.3636 #hcsteps: 2.65909
#peggingImprovements: 0
D4 table contains 78561 parameters.
A/D table contains 19565 parameters.
A/D table contains 17036 parameters.
NTable contains 1570 parameter.
p0_count is 1116.13 and p1 is 24.4358; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 1.71167 PERPLEXITY 3.2754
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 1.73825 PERPLEXITY 3.3363

T3To4 Viterbi Iteration : 4 took: 0 seconds

---------------------
Model4: Iteration 5
#centers(pre/hillclimbed/real): 1 1 1  #al: 1083.93 #alsophisticatedcountcollection: 14.9545 #hcsteps: 2.79545
#peggingImprovements: 0
D4 table contains 78561 parameters.
A/D table contains 19565 parameters.
A/D table contains 17036 parameters.
NTable contains 1570 parameter.
p0_count is 1120.26 and p1 is 22.369; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 1.96836 PERPLEXITY 3.91323
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 1.98799 PERPLEXITY 3.96683

Model4 Viterbi Iteration : 5 took: 0 seconds

---------------------
Model4: Iteration 6
#centers(pre/hillclimbed/real): 1 1 1  #al: 1083.8 #alsophisticatedcountcollection: 12.1364 #hcsteps: 2.86364
#peggingImprovements: 0
D4 table contains 78561 parameters.
A/D table contains 19565 parameters.
A/D table contains 17017 parameters.
NTable contains 1570 parameter.
p0_count is 1131.79 and p1 is 16.6064; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 1.8663 PERPLEXITY 3.64596
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 1.87561 PERPLEXITY 3.66956

Model4 Viterbi Iteration : 6 took: 0 seconds
H333444 Training Finished at: Sun Apr 23 12:17:49 2017


Entire Viterbi H333444 Training took: 1 seconds
==========================================================

Entire Training took: 1 seconds
Program Finished at: Sun Apr 23 12:17:49 2017

==========================================================
Executing: rm -f /home/hgkumbhare/working/train/giza.fr-en/fr-en.A3.final.gz
Executing: gzip /home/hgkumbhare/working/train/giza.fr-en/fr-en.A3.final
(2.1a) running snt2cooc en-fr @ Sun Apr 23 12:17:49 IST 2017

Executing: mkdir -p /home/hgkumbhare/working/train/giza.en-fr
Executing: /home/hgkumbhare/mosesdecoder/tools/snt2cooc.out /home/hgkumbhare/working/train/corpus/fr.vcb /home/hgkumbhare/working/train/corpus/en.vcb /home/hgkumbhare/working/train/corpus/en-fr-int-train.snt > /home/hgkumbhare/working/train/giza.en-fr/en-fr.cooc
/home/hgkumbhare/mosesdecoder/tools/snt2cooc.out /home/hgkumbhare/working/train/corpus/fr.vcb /home/hgkumbhare/working/train/corpus/en.vcb /home/hgkumbhare/working/train/corpus/en-fr-int-train.snt > /home/hgkumbhare/working/train/giza.en-fr/en-fr.cooc
END.
(2.1b) running giza en-fr @ Sun Apr 23 12:17:49 IST 2017
/home/hgkumbhare/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/hgkumbhare/working/train/giza.en-fr/en-fr.cooc -c /home/hgkumbhare/working/train/corpus/en-fr-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/hgkumbhare/working/train/giza.en-fr/en-fr -onlyaldumps 1 -p0 0.999 -s /home/hgkumbhare/working/train/corpus/fr.vcb -t /home/hgkumbhare/working/train/corpus/en.vcb
Executing: /home/hgkumbhare/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/hgkumbhare/working/train/giza.en-fr/en-fr.cooc -c /home/hgkumbhare/working/train/corpus/en-fr-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/hgkumbhare/working/train/giza.en-fr/en-fr -onlyaldumps 1 -p0 0.999 -s /home/hgkumbhare/working/train/corpus/fr.vcb -t /home/hgkumbhare/working/train/corpus/en.vcb
/home/hgkumbhare/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/hgkumbhare/working/train/giza.en-fr/en-fr.cooc -c /home/hgkumbhare/working/train/corpus/en-fr-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/hgkumbhare/working/train/giza.en-fr/en-fr -onlyaldumps 1 -p0 0.999 -s /home/hgkumbhare/working/train/corpus/fr.vcb -t /home/hgkumbhare/working/train/corpus/en.vcb
Parameter 'coocurrencefile' changed from '' to '/home/hgkumbhare/working/train/giza.en-fr/en-fr.cooc'
Parameter 'c' changed from '' to '/home/hgkumbhare/working/train/corpus/en-fr-int-train.snt'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '117-04-23.121749.hgkumbhare' to '/home/hgkumbhare/working/train/giza.en-fr/en-fr'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to '/home/hgkumbhare/working/train/corpus/fr.vcb'
Parameter 't' changed from '' to '/home/hgkumbhare/working/train/corpus/en.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 117-04-23.121749.hgkumbhare.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/hgkumbhare/working/train/giza.en-fr/en-fr  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/hgkumbhare/working/train/corpus/en-fr-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/hgkumbhare/working/train/corpus/fr.vcb  (source vocabulary file name)
t = /home/hgkumbhare/working/train/corpus/en.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 117-04-23.121749.hgkumbhare.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/hgkumbhare/working/train/giza.en-fr/en-fr  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/hgkumbhare/working/train/corpus/en-fr-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/hgkumbhare/working/train/corpus/fr.vcb  (source vocabulary file name)
t = /home/hgkumbhare/working/train/corpus/en.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:/home/hgkumbhare/working/train/corpus/fr.vcb
Reading vocabulary file from:/home/hgkumbhare/working/train/corpus/en.vcb
Source vocabulary list has 147 unique tokens 
Target vocabulary list has 157 unique tokens 
Calculating vocabulary frequencies from corpus /home/hgkumbhare/working/train/corpus/en-fr-int-train.snt
Reading more sentence pairs into memory ... 
Corpus fits in memory, corpus has: 44 sentence pairs.
 Train total # sentence pairs (weighted): 44
Size of source portion of the training corpus: 1165 tokens
Size of the target portion of the training corpus: 1068 tokens 
In source portion of the training corpus, only 146 unique tokens appeared
In target portion of the training corpus, only 155 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 1068/(1209-44)== 0.916738
There are 5599 5599 entries in table
==========================================================
Model1 Training Started at: Sun Apr 23 12:17:49 2017

-----------
Model1: Iteration 1
Model1: (1) TRAIN CROSS-ENTROPY 7.47811 PERPLEXITY 178.293
Model1: (1) VITERBI TRAIN CROSS-ENTROPY 12.3406 PERPLEXITY 5186.59
Model 1 Iteration: 1 took: 0 seconds
-----------
Model1: Iteration 2
Model1: (2) TRAIN CROSS-ENTROPY 5.21048 PERPLEXITY 37.0265
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 8.9874 PERPLEXITY 507.549
Model 1 Iteration: 2 took: 0 seconds
-----------
Model1: Iteration 3
Model1: (3) TRAIN CROSS-ENTROPY 4.97032 PERPLEXITY 31.3484
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 8.34329 PERPLEXITY 324.774
Model 1 Iteration: 3 took: 0 seconds
-----------
Model1: Iteration 4
Model1: (4) TRAIN CROSS-ENTROPY 4.82944 PERPLEXITY 28.4319
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 7.81086 PERPLEXITY 224.544
Model 1 Iteration: 4 took: 0 seconds
-----------
Model1: Iteration 5
Model1: (5) TRAIN CROSS-ENTROPY 4.73814 PERPLEXITY 26.6885
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 7.41434 PERPLEXITY 170.584
Model 1 Iteration: 5 took: 0 seconds
Entire Model1 Training took: 0 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 146  #classes: 51
Read classes: #words: 156  #classes: 51

==========================================================
Hmm Training Started at: Sun Apr 23 12:17:49 2017

-----------
Hmm: Iteration 1
A/D table contains 17009 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 4.67567 PERPLEXITY 25.5575
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY 7.11746 PERPLEXITY 138.857

Hmm Iteration: 1 took: 0 seconds

-----------
Hmm: Iteration 2
A/D table contains 17009 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 4.368 PERPLEXITY 20.649
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 5.54325 PERPLEXITY 46.632

Hmm Iteration: 2 took: 0 seconds

-----------
Hmm: Iteration 3
A/D table contains 17009 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 3.72376 PERPLEXITY 13.2118
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 4.36944 PERPLEXITY 20.6696

Hmm Iteration: 3 took: 0 seconds

-----------
Hmm: Iteration 4
A/D table contains 17009 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 2.64872 PERPLEXITY 6.2711
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 2.86712 PERPLEXITY 7.29609

Hmm Iteration: 4 took: 0 seconds

-----------
Hmm: Iteration 5
A/D table contains 17009 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 1.81042 PERPLEXITY 3.50745
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 1.88831 PERPLEXITY 3.70202

Hmm Iteration: 5 took: 0 seconds

Entire Hmm Training took: 0 seconds
==========================================================
Read classes: #words: 146  #classes: 51
Read classes: #words: 156  #classes: 51
Read classes: #words: 146  #classes: 51
Read classes: #words: 156  #classes: 51

==========================================================
Starting H333444:  Viterbi Training
 H333444 Training Started at: Sun Apr 23 12:17:49 2017


---------------------
THTo3: Iteration 1
#centers(pre/hillclimbed/real): 1 1 1  #al: 1033.64 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 17009 parameters.
A/D table contains 19271 parameters.
NTable contains 1470 parameter.
p0_count is 960.031 and p1 is 53.9843; p0 is 0.999 p1: 0.001
THTo3: TRAIN CROSS-ENTROPY 1.39144 PERPLEXITY 2.6234
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 1.42088 PERPLEXITY 2.67749

THTo3 Viterbi Iteration : 1 took: 0 seconds

---------------------
Model3: Iteration 2
#centers(pre/hillclimbed/real): 1 1 1  #al: 1034.36 #alsophisticatedcountcollection: 0 #hcsteps: 2.31818
#peggingImprovements: 0
A/D table contains 17009 parameters.
A/D table contains 19258 parameters.
NTable contains 1470 parameter.
p0_count is 1028.19 and p1 is 19.9064; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 1.59517 PERPLEXITY 3.0213
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 1.63156 PERPLEXITY 3.09849

Model3 Viterbi Iteration : 2 took: 0 seconds

---------------------
Model3: Iteration 3
#centers(pre/hillclimbed/real): 1 1 1  #al: 1034.43 #alsophisticatedcountcollection: 0 #hcsteps: 2.61364
#peggingImprovements: 0
A/D table contains 17009 parameters.
A/D table contains 19248 parameters.
NTable contains 1470 parameter.
p0_count is 1043.31 and p1 is 12.3434; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 1.44165 PERPLEXITY 2.71632
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 1.46929 PERPLEXITY 2.76886

Model3 Viterbi Iteration : 3 took: 0 seconds

---------------------
T3To4: Iteration 4
#centers(pre/hillclimbed/real): 1 1 1  #al: 1034.48 #alsophisticatedcountcollection: 10.8636 #hcsteps: 2.63636
#peggingImprovements: 0
D4 table contains 80388 parameters.
A/D table contains 17009 parameters.
A/D table contains 19203 parameters.
NTable contains 1470 parameter.
p0_count is 1048.5 and p1 is 9.75011; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 1.39261 PERPLEXITY 2.62553
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 1.41755 PERPLEXITY 2.67132

T3To4 Viterbi Iteration : 4 took: 0 seconds

---------------------
Model4: Iteration 5
#centers(pre/hillclimbed/real): 1 1 1  #al: 1034.5 #alsophisticatedcountcollection: 9.77273 #hcsteps: 2.52273
#peggingImprovements: 0
D4 table contains 80388 parameters.
A/D table contains 17009 parameters.
A/D table contains 19203 parameters.
NTable contains 1470 parameter.
p0_count is 1047.75 and p1 is 10.1256; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 1.91168 PERPLEXITY 3.76248
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 1.92466 PERPLEXITY 3.79647

Model4 Viterbi Iteration : 5 took: 0 seconds

---------------------
Model4: Iteration 6
#centers(pre/hillclimbed/real): 1 1 1  #al: 1034.52 #alsophisticatedcountcollection: 8.56818 #hcsteps: 2.54545
#peggingImprovements: 0
D4 table contains 80388 parameters.
A/D table contains 17009 parameters.
A/D table contains 19132 parameters.
NTable contains 1470 parameter.
p0_count is 1047.29 and p1 is 10.3566; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 1.87463 PERPLEXITY 3.66706
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 1.88739 PERPLEXITY 3.69964

Model4 Viterbi Iteration : 6 took: 0 seconds
H333444 Training Finished at: Sun Apr 23 12:17:49 2017


Entire Viterbi H333444 Training took: 0 seconds
==========================================================

Entire Training took: 0 seconds
Program Finished at: Sun Apr 23 12:17:49 2017

==========================================================
Executing: rm -f /home/hgkumbhare/working/train/giza.en-fr/en-fr.A3.final.gz
Executing: gzip /home/hgkumbhare/working/train/giza.en-fr/en-fr.A3.final
(3) generate word alignment @ Sun Apr 23 12:17:49 IST 2017
Combining forward and inverted alignment from files:
  /home/hgkumbhare/working/train/giza.fr-en/fr-en.A3.final.{bz2,gz}
  /home/hgkumbhare/working/train/giza.en-fr/en-fr.A3.final.{bz2,gz}
Executing: mkdir -p /home/hgkumbhare/working/train/model
Executing: /home/hgkumbhare/mosesdecoder/scripts/training/giza2bal.pl -d "gzip -cd /home/hgkumbhare/working/train/giza.en-fr/en-fr.A3.final.gz" -i "gzip -cd /home/hgkumbhare/working/train/giza.fr-en/fr-en.A3.final.gz" |/home/hgkumbhare/mosesdecoder/scripts/../bin/symal -alignment="grow" -diagonal="yes" -final="yes" -both="no" > /home/hgkumbhare/working/train/model/aligned.grow-diag-final
symal: computing grow alignment: diagonal (1) final (1)both-uncovered (0)
skip=<0> counts=<44>
(4) generate lexical translation table 0-0 @ Sun Apr 23 12:17:49 IST 2017
(/home/hgkumbhare/corpus/nl_file_lowercase.fr,/home/hgkumbhare/corpus/nl_file_lowercase.en,/home/hgkumbhare/working/train/model/lex)
!
Saved: /home/hgkumbhare/working/train/model/lex.f2e and /home/hgkumbhare/working/train/model/lex.e2f
FILE: /home/hgkumbhare/corpus/nl_file_lowercase.en
FILE: /home/hgkumbhare/corpus/nl_file_lowercase.fr
FILE: /home/hgkumbhare/working/train/model/aligned.grow-diag-final
(5) extract phrases @ Sun Apr 23 12:17:49 IST 2017
/home/hgkumbhare/mosesdecoder/scripts/generic/extract-parallel.perl 4 split "sort    " /home/hgkumbhare/mosesdecoder/scripts/../bin/extract-rules /home/hgkumbhare/corpus/nl_file_lowercase.en /home/hgkumbhare/corpus/nl_file_lowercase.fr /home/hgkumbhare/working/train/model/aligned.grow-diag-final /home/hgkumbhare/working/train/model/extract --GlueGrammar /home/hgkumbhare/working/train/model/glue-grammar --MaxSpan 10 --GZOutput 
Executing: /home/hgkumbhare/mosesdecoder/scripts/generic/extract-parallel.perl 4 split "sort    " /home/hgkumbhare/mosesdecoder/scripts/../bin/extract-rules /home/hgkumbhare/corpus/nl_file_lowercase.en /home/hgkumbhare/corpus/nl_file_lowercase.fr /home/hgkumbhare/working/train/model/aligned.grow-diag-final /home/hgkumbhare/working/train/model/extract --GlueGrammar /home/hgkumbhare/working/train/model/glue-grammar --MaxSpan 10 --GZOutput 
Started Sun Apr 23 12:17:49 2017
using gzip 
isBSDSplit=0 
Executing: mkdir -p /home/hgkumbhare/working/train/model/tmp.6892; ls -l /home/hgkumbhare/working/train/model/tmp.6892 
total=44 line-per-split=12 
split -d -l 12 -a 7 /home/hgkumbhare/corpus/nl_file_lowercase.en /home/hgkumbhare/working/train/model/tmp.6892/target.split -d -l 12 -a 7 /home/hgkumbhare/corpus/nl_file_lowercase.fr /home/hgkumbhare/working/train/model/tmp.6892/source.split -d -l 12 -a 7 /home/hgkumbhare/working/train/model/aligned.grow-diag-final /home/hgkumbhare/working/train/model/tmp.6892/align.merging extract / extract.inv
gunzip -c /home/hgkumbhare/working/train/model/tmp.6892/extract.0000000.gz /home/hgkumbhare/working/train/model/tmp.6892/extract.0000001.gz /home/hgkumbhare/working/train/model/tmp.6892/extract.0000002.gz /home/hgkumbhare/working/train/model/tmp.6892/extract.0000003.gz  | LC_ALL=C sort     -T /home/hgkumbhare/working/train/model/tmp.6892 2>> /dev/stderr | gzip -c > /home/hgkumbhare/working/train/model/extract.sorted.gz 2>> /dev/stderr 
gunzip -c /home/hgkumbhare/working/train/model/tmp.6892/extract.0000000.inv.gz /home/hgkumbhare/working/train/model/tmp.6892/extract.0000001.inv.gz /home/hgkumbhare/working/train/model/tmp.6892/extract.0000002.inv.gz /home/hgkumbhare/working/train/model/tmp.6892/extract.0000003.inv.gz  | LC_ALL=C sort     -T /home/hgkumbhare/working/train/model/tmp.6892 2>> /dev/stderr | gzip -c > /home/hgkumbhare/working/train/model/extract.inv.sorted.gz 2>> /dev/stderr 
Merging glue rules: cat /home/hgkumbhare/working/train/model/tmp.6892/glue.* | LC_ALL=C sort | uniq > /home/hgkumbhare/working/train/model/glue-grammar 
Finished Sun Apr 23 12:17:50 2017
(6) score phrases @ Sun Apr 23 12:17:50 IST 2017
(6.1)  creating table half /home/hgkumbhare/working/train/model/rule-table.half.f2e @ Sun Apr 23 12:17:50 IST 2017
/home/hgkumbhare/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/hgkumbhare/mosesdecoder/scripts/../bin/score /home/hgkumbhare/working/train/model/extract.sorted.gz /home/hgkumbhare/working/train/model/lex.f2e /home/hgkumbhare/working/train/model/rule-table.half.f2e.gz  --Hierarchical 0 
Executing: /home/hgkumbhare/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/hgkumbhare/mosesdecoder/scripts/../bin/score /home/hgkumbhare/working/train/model/extract.sorted.gz /home/hgkumbhare/working/train/model/lex.f2e /home/hgkumbhare/working/train/model/rule-table.half.f2e.gz  --Hierarchical 0 
using gzip 
Started Sun Apr 23 12:17:50 2017
/home/hgkumbhare/mosesdecoder/scripts/../bin/score /home/hgkumbhare/working/train/model/tmp.6939/extract.0.gz /home/hgkumbhare/working/train/model/lex.f2e /home/hgkumbhare/working/train/model/tmp.6939/phrase-table.half.0000000.gz --Hierarchical  2>> /dev/stderr 
/home/hgkumbhare/working/train/model/tmp.6939/run.0.sh/home/hgkumbhare/working/train/model/tmp.6939/run.1.sh/home/hgkumbhare/working/train/model/tmp.6939/run.2.sh/home/hgkumbhare/working/train/model/tmp.6939/run.3.shmv /home/hgkumbhare/working/train/model/tmp.6939/phrase-table.half.0000000.gz /home/hgkumbhare/working/train/model/rule-table.half.f2e.gzrm -rf /home/hgkumbhare/working/train/model/tmp.6939 
Finished Sun Apr 23 12:17:51 2017
(6.3)  creating table half /home/hgkumbhare/working/train/model/rule-table.half.e2f @ Sun Apr 23 12:17:51 IST 2017
/home/hgkumbhare/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/hgkumbhare/mosesdecoder/scripts/../bin/score /home/hgkumbhare/working/train/model/extract.inv.sorted.gz /home/hgkumbhare/working/train/model/lex.e2f /home/hgkumbhare/working/train/model/rule-table.half.e2f.gz --Inverse --Hierarchical 1 
Executing: /home/hgkumbhare/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/hgkumbhare/mosesdecoder/scripts/../bin/score /home/hgkumbhare/working/train/model/extract.inv.sorted.gz /home/hgkumbhare/working/train/model/lex.e2f /home/hgkumbhare/working/train/model/rule-table.half.e2f.gz --Inverse --Hierarchical 1 
using gzip 
Started Sun Apr 23 12:17:51 2017
/home/hgkumbhare/mosesdecoder/scripts/../bin/score /home/hgkumbhare/working/train/model/tmp.6967/extract.0.gz /home/hgkumbhare/working/train/model/lex.e2f /home/hgkumbhare/working/train/model/tmp.6967/phrase-table.half.0000000.gz --Inverse --Hierarchical  2>> /dev/stderr 
/home/hgkumbhare/working/train/model/tmp.6967/run.0.sh/home/hgkumbhare/working/train/model/tmp.6967/run.1.sh/home/hgkumbhare/working/train/model/tmp.6967/run.2.sh/home/hgkumbhare/working/train/model/tmp.6967/run.3.shgunzip -c /home/hgkumbhare/working/train/model/tmp.6967/phrase-table.half.*.gz 2>> /dev/stderr| LC_ALL=C sort     -T /home/hgkumbhare/working/train/model/tmp.6967  | gzip -c > /home/hgkumbhare/working/train/model/rule-table.half.e2f.gz  2>> /dev/stderr rm -rf /home/hgkumbhare/working/train/model/tmp.6967 
Finished Sun Apr 23 12:17:51 2017
(6.6) consolidating the two halves @ Sun Apr 23 12:17:51 IST 2017
Executing: /home/hgkumbhare/mosesdecoder/scripts/../bin/consolidate /home/hgkumbhare/working/train/model/rule-table.half.f2e.gz /home/hgkumbhare/working/train/model/rule-table.half.e2f.gz /dev/stdout --Hierarchical | gzip -c > /home/hgkumbhare/working/train/model/rule-table.gz
Consolidate v2.0 written by Philipp Koehn
consolidating direct and indirect rule tables
processing hierarchical rules

Executing: rm -f /home/hgkumbhare/working/train/model/rule-table.half.*
(7) learn reordering model @ Sun Apr 23 12:17:52 IST 2017
  ... skipping this step, reordering is not lexicalized ...
(8) learn generation model @ Sun Apr 23 12:17:52 IST 2017
  no generation model requested, skipping step
(9) create moses.ini @ Sun Apr 23 12:17:52 IST 2017
