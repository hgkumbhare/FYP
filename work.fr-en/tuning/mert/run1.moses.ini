# MERT optimized configuration
# decoder /home/hgkumbhare/mosesdecoder/bin/moses_chart
# BLEU --not-estimated-- on dev /home/hgkumbhare/corpus/nl_file_lowercase.fr
# We were before running iteration 1
# finished Tue May  2 13:49:09 IST 2017
### MOSES CONFIG FILE ###
#########################

# input factors
[input-factors]
0

# mapping steps
[mapping]
0 T 0
1 T 1

[cube-pruning-pop-limit]
1000

[non-terminals]
X

[search-algorithm]
3

[inputtype]
3

[feature]
UnknownWordPenalty
WordPenalty
PhrasePenalty
PhraseDictionaryMemory name=TranslationModel0 num-features=4 path=/home/hgkumbhare/working/work.fr-en/training/model.filtered/dev/phrase-table.0-0.1.1.gz input-factor=0 output-factor=0 
PhraseDictionaryMemory name=TranslationModel1 num-features=1 path=/home/hgkumbhare/working/work.fr-en/training/model/glue-grammar input-factor=0 output-factor=0 tuneable=true
KENLM name=LM0 factor=0 path=/home/hgkumbhare/lm/nl_file.blm.en order=3

# dense weights for feature functions

[max-chart-span]
100

[threads]
16
[weight]

LM0= 0.500000
WordPenalty0= -1.000000
PhrasePenalty0= 0.200000
TranslationModel0= 0.200000 0.200000 0.200000 0.200000
TranslationModel1= 1.000000
UnknownWordPenalty0= 1
